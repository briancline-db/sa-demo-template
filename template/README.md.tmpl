# AI-Assisted Databricks Demo Factory Template

## ðŸŽ¯ Overview

This template enables Solution Architects to create sophisticated, presentation-ready Databricks demos using natural language descriptions and AI assistance. The template provides a complete foundation for building industry-specific analytics dashboards with professional Databricks branding.

## ðŸš€ Quick Start

### 1. Initialize Your Demo
```bash
# Create a new demo project
mkdir my-retail-demo
cd my-retail-demo

# Copy template files
cp -r template/* .

# Edit demo requirements
nano demo-requirements.md
```

### 2. Open in Cursor
```bash
cursor .
```

### 3. Let AI Build Your Demo
The AI will read your `demo-requirements.md` and implement:
- Industry-specific synthetic data generation
- Professional React dashboard with Databricks styling
- API endpoints for data consumption
- Advanced features based on complexity level

## ðŸ“‹ Template Structure

```
template/
â”œâ”€â”€ databricks_template_schema.json     # Enhanced user prompts
â”œâ”€â”€ databricks.yml.tmpl                # Asset Bundle config
â”œâ”€â”€ pyproject.toml.tmpl                # Python setup
â”œâ”€â”€ demo-requirements.md.tmpl          # Demo description template
â”œâ”€â”€ .cursor/rules/                     # ðŸŽ¯ SOPHISTICATED CURSOR RULES
â”‚   â”œâ”€â”€ demo-builder.md               # Main AI demo builder rules
â”‚   â”œâ”€â”€ databricks-expert.md          # Databricks best practices
â”‚   â”œâ”€â”€ react-styling.md              # React + Tailwind + shadcn rules
â”‚   â””â”€â”€ data-generation.md            # Synthetic data patterns
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ {{.project_name}}/            # Python data generation
â”‚   â”‚   â””â”€â”€ main.py.tmpl              # Base data generation
â”‚   â””â”€â”€ app/                          # ðŸŽ¯ BASE REACT APP
â”‚       â”œâ”€â”€ package.json.tmpl         # Dependencies
â”‚       â”œâ”€â”€ app.js.tmpl               # Express server
â”‚       â”œâ”€â”€ tailwind.config.js.tmpl   # Databricks design tokens
â”‚       â”œâ”€â”€ components/               # Component library
â”‚       â”‚   â”œâ”€â”€ databricks/           # Databricks-styled components
â”‚       â”‚   â””â”€â”€ charts/               # Chart components
â”‚       â””â”€â”€ public/
â”‚           â””â”€â”€ index.html.tmpl       # Main dashboard
â”œâ”€â”€ resources/                         # Bundle resources
â”‚   â”œâ”€â”€ {{.project_name}}.job.yml.tmpl
â”‚   â””â”€â”€ {{.project_name}}.app.yml.tmpl
â””â”€â”€ docs/
    â”œâ”€â”€ cursor-usage.md               # How to use Cursor
    â””â”€â”€ databricks-patterns.md        # Best practices reference
```

## ðŸŽ¨ Features

### Industry Support
- **Retail**: Customer analytics, inventory management, sales forecasting
- **Supply Chain**: Supplier performance, logistics tracking, risk assessment
- **Finance**: Portfolio analytics, fraud detection, regulatory compliance
- **Healthcare**: Patient analytics, clinical outcomes, operational efficiency
- **Manufacturing**: Production analytics, quality control, predictive maintenance
- **Telecom**: Network performance, customer churn, service quality

### Complexity Levels
- **Simple**: Basic dashboards with KPIs and charts
- **Standard**: Dashboards + OLTP integration for real-time data
- **Advanced**: Dashboards + OLTP + LLM agents for conversational analytics

### Design System
- **Databricks Branding**: Official colors, typography, and components
- **Professional UI**: Modern, executive-ready interfaces
- **Responsive Design**: Works on all screen sizes
- **Interactive Elements**: Filters, drill-downs, real-time updates

## ðŸ“Š Data Generation

### Synthetic Data Features
- **Realistic Distributions**: Industry-appropriate statistical patterns
- **Referential Integrity**: Proper foreign key relationships
- **Scalable Volumes**: Small (1K), Medium (10K), Large (100K) records
- **Time Series Data**: Historical trends and seasonal patterns
- **Quality Assurance**: Data validation and error handling

### Supported Data Types
- Customer demographics and behavior
- Product catalogs and inventory
- Transaction history and analytics
- Geographic and location data
- Time-series and trend data

## ðŸ”§ Technical Stack

### Backend
- **Python + PySpark**: Data generation and processing
- **dbldatagen**: Synthetic data generation
- **Databricks SDK**: Platform integration
- **Unity Catalog**: Data governance

### Frontend
- **React**: Modern UI framework
- **Tailwind CSS**: Utility-first styling
- **Recharts**: Interactive visualizations
- **Express.js**: API server

### Infrastructure
- **Databricks Asset Bundle**: Unified deployment
- **Delta Lake**: Data storage and optimization
- **MLflow**: Model lifecycle management
- **Lakebase**: OLTP integration (optional)

## ðŸš€ Deployment

### 1. Data Generation
```bash
# Generate synthetic data
databricks bundle run {{.project_name}}_job --target dev
```

### 2. Dashboard Deployment
```bash
# Deploy dashboard
databricks bundle deploy --target dev
```

### 3. Access Dashboard
```bash
# Open dashboard URL
open https://your-workspace.cloud.databricks.com/apps/{{.project_name}}_app
```

## ðŸŽ¯ Usage Examples

### Retail Analytics Demo
```markdown
## ðŸ“‹ Detailed Requirements
A comprehensive retail analytics platform showing customer behavior, inventory optimization, and sales forecasting. Include real-time transaction monitoring, customer segmentation with ML, and predictive analytics for demand planning. Create an executive dashboard with KPIs and drill-down capabilities.
```

### Supply Chain Demo
```markdown
## ðŸ“‹ Detailed Requirements
A supply chain optimization platform tracking supplier performance, logistics efficiency, and risk management. Include real-time shipment tracking, supplier analytics, and predictive modeling for demand forecasting. Create a dashboard with geographic visualizations and interactive filters.
```

### Finance Demo
```markdown
## ðŸ“‹ Detailed Requirements
A financial analytics platform for portfolio management, risk assessment, and fraud detection. Include real-time transaction monitoring, customer lifetime value analysis, and predictive risk modeling. Create an executive dashboard with regulatory compliance reporting.
```

## ðŸ¤– AI Assistant Features

### Intelligent Implementation
- **Context Awareness**: Understands industry patterns and requirements
- **Best Practices**: Follows Databricks platform standards
- **Quality Assurance**: Implements proper error handling and validation
- **Performance Optimization**: Optimizes for fast loading and smooth interactions

### Customization Support
- **Component Library**: Reusable Databricks-styled components
- **Chart Library**: Interactive visualizations with proper theming
- **API Patterns**: Standardized endpoint structure
- **Data Patterns**: Industry-specific data generation templates

## ðŸ“š Documentation

- **[Cursor Usage Guide](docs/cursor-usage.md)**: How to work with Cursor AI
- **[Databricks Patterns](docs/databricks-patterns.md)**: Best practices and standards
- **[Component Library](src/app/components/)**: Reusable UI components
- **[Data Generation](src/{{.project_name}}/main.py.tmpl)**: Synthetic data patterns

## ðŸŽ¯ Success Metrics

### Professional Quality
- âœ… **Executive Ready**: Professional presentation quality
- âœ… **Fast Loading**: < 3 second load times
- âœ… **Responsive Design**: Works on all devices
- âœ… **Interactive**: Engaging user experience

### Technical Excellence
- âœ… **Databricks Native**: Uses platform capabilities effectively
- âœ… **Scalable**: Handles data growth gracefully
- âœ… **Maintainable**: Clean, well-documented code
- âœ… **Deployable**: 5-minute setup and deployment

### Business Value
- âœ… **Actionable Insights**: Clear business recommendations
- âœ… **Compelling Story**: Data tells a business narrative
- âœ… **Customer Focus**: Addresses real business problems
- âœ… **ROI Demonstration**: Shows clear value proposition

## ðŸ”„ Development Workflow

### 1. Requirements Definition
- Write detailed demo requirements in `demo-requirements.md`
- Specify industry, complexity, and key features
- Include business context and target audience

### 2. AI-Assisted Implementation
- Open project in Cursor
- AI reads requirements and proposes implementation
- Iterate with feedback and refinements
- Test functionality and performance

### 3. Deployment and Presentation
- Deploy to Databricks workspace
- Test all features and interactions
- Prepare presentation narrative
- Demo to customers and stakeholders

## ðŸ’¡ Best Practices

### For Solution Architects
1. **Be Specific**: Detailed requirements lead to better results
2. **Include Context**: Business background helps AI understand needs
3. **Specify Complexity**: Choose appropriate feature level
4. **Focus on Value**: Emphasize business impact over technical details
5. **Test Thoroughly**: Validate before customer presentations

### For AI Implementation
1. **Start Simple**: Build MVP first, enhance incrementally
2. **Follow Patterns**: Use established component and data patterns
3. **Optimize Performance**: Ensure fast loading and smooth interactions
4. **Polish Presentation**: Professional appearance is crucial
5. **Document Decisions**: Explain technical choices and trade-offs

## ðŸš€ Getting Help

### Documentation
- Review the [Cursor Usage Guide](docs/cursor-usage.md) for AI interaction tips
- Check [Databricks Patterns](docs/databricks-patterns.md) for best practices
- Explore component examples in the [source code](src/)

### Common Issues
- **Data Generation**: Check Unity Catalog permissions and cluster configuration
- **Dashboard Loading**: Verify API endpoints and data connectivity
- **Styling Issues**: Ensure Tailwind CSS is properly configured
- **Deployment Errors**: Check Asset Bundle configuration and workspace access

### Support
- Review template documentation and examples
- Check Databricks documentation for platform-specific issues
- Consult with your Databricks team for workspace configuration

---

**Built with â¤ï¸ for Databricks Solution Architects**

This template represents the future of demo creation: fast, intelligent, and professional. Every demo tells a story, every interaction matters, and every detail contributes to customer success. 